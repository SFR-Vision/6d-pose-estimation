# Project Status - 6D Pose Estimation (LineMOD Dataset)

**Last Updated**: December 16, 2025  
**Current Phase**: Production Ready - Colab Deployment Complete

---

## ðŸŽ¯ Project Goal

Estimate 6D pose (3D rotation + 3D translation) of objects from RGB-D images using the LineMOD dataset (13 objects).

**Performance Metric**: ADD (Average Distance of Model Points) - lower is better  
**Target**: < 5cm ADD error

---

## ðŸ“Š Final Results

| Model | Architecture | ADD Error (Test) | ADD-S Accuracy @50mm | Status |
|-------|-------------|------------------|---------------------|--------|
| **RGB-only** | ResNet50 | **50.3mm** | 52.6% | âœ… Trained (10 epochs) |
| **Hybrid** â­ | ResNet50 + Custom CNN + Pinhole | **47.7mm** | 58.9% | âœ… **Trained (Best)** |
| RGB-D | ResNet50 + ResNet50 | Not trained | - | â¸ï¸ Pending retraining |

**Key Finding**: Hybrid model achieves **5.2% better accuracy** than RGB-only by incorporating camera geometry!

---

## ðŸš€ Current Status - PRODUCTION READY âœ…

**Completed**:
- âœ… RGB model trained (10 epochs) - 50.3mm test ADD error
- âœ… Hybrid model trained (full training) - 47.7mm test ADD error
- âœ… Comparison analysis with detailed metrics (ADD-S accuracy, distribution stats)
- âœ… Colab notebook with "Run All" automation
- âœ… Pre-trained weights uploaded to Google Drive
- âœ… Comprehensive visualizations (3D bounding boxes, side-by-side comparisons)
- âœ… `.github/copilot-instructions.md` for AI agent onboarding

**Ready for Deployment**:
- ðŸŽ¯ Teammates can run `colab_setup.ipynb` â†’ "Run All" â†’ Get results in 20-30 minutes
- ðŸ“¦ Pre-trained weights download automatically (RGB + Hybrid + YOLO)
- ðŸ“Š All visualizations generate automatically
- ðŸ’¾ Results save to Google Drive automatically

**Future Work**:
- ðŸ”„ Retrain RGB-D model with fixed augmentation (currently deprioritized)
- ðŸŽ“ Explore deeper architectures (ResNet101, EfficientNet)
- ðŸŒ Test on other datasets (YCB-Video, T-LESS)

---

## ðŸ—ï¸ Architecture Overview

### Model 1: RGB-only (`pose_net_rgb.py`)
- **Input**: RGB image (224Ã—224)
- **Backbone**: ResNet50 (pretrained)
- **Heads**: Rotation (4 outputs - quaternion) + Translation (3 outputs - XYZ)
- **Parameters**: ~25M
- **Files**: `dataset_rgb.py`, `train_rgb.py`, `inference_rgb.py`, `visualize_rgb.py`

### Model 2: RGB-D (`pose_net_rgbd.py`)
- **Input**: RGB (224Ã—224) + Depth (224Ã—224)
- **Backbone**: ResNet50 RGB + ResNet50 Depth (upgraded from ResNet18)
- **Fusion**: Concatenate features â†’ 2048+2048 â†’ 2048
- **Heads**: Rotation (4) + Translation (3)
- **Parameters**: ~50M
- **Files**: `dataset_rgbd.py`, `train_rgbd.py`, `inference_rgbd.py`, `visualize_rgbd.py`

### Model 3: Hybrid (`pose_net_hybrid.py`) â­ **BEST PERFORMER**
- **Input**: RGB (224Ã—224) + Bbox Center + Camera Matrix (Note: Depth NOT used)
- **Rotation Branch**: ResNet50 (pretrained) â†’ 2048 â†’ 1024 â†’ 512 â†’ 4 (quaternion) - **LEARNED**
- **Z-Depth Branch**: Custom CNN (from RGB, not depth!)
  - 224Ã—224Ã—3 â†’ Conv7+Pool â†’ Conv5+Pool â†’ Conv3+Pool â†’ Conv3+Pool â†’ GlobalAvgPool â†’ 256 features
  - 256 â†’ 128 â†’ 64 â†’ 1 (Z distance in meters) - **LEARNED**
- **X,Y Translation**: Pinhole camera model - **GEOMETRIC** (not learned)
  - `X = (u - cx) * Z / fx`
  - `Y = (v - cy) * Z / fy`
- **Parameters**: ~30M (5 learned outputs vs 7 for RGB-only)
- **Performance**: 47.7mm ADD error (5% better than RGB-only)
- **Files**: `dataset_hybrid.py`, `train_hybrid.py`, `compare_rgb_vs_hybrid.py`, `visualize_hybrid.py`

**Why Hybrid?**: Tests hypothesis that incorporating domain knowledge (camera geometry) improves over pure learning.

---

## ðŸ› Critical Bug Fixed

**MAJOR BUG DISCOVERED & FIXED**:
- **Issue**: Original augmentation added rotation/translation **noise to labels WITHOUT modifying images**
- **Impact**: Model trained on corrupted labels â†’ worse performance
- **Fix**: Removed rotation (Â±5Â°) and translation (Â±20mm) noise, kept only bbox jitter
- **Files affected**: `dataset_rgb.py`, `dataset_rgbd.py`, `dataset_hybrid.py`
- **Parameter renamed**: `augment_pose` â†’ `augment_bbox` (more accurate name)

**Result**: All models need retraining with fixed augmentation after hybrid experiments.

---

## ðŸ”§ Windows-Specific Fixes Applied

### Fix 1: Multiprocessing Bootstrap Error
- **Problem**: DataLoader workers re-executed main script on spawn
- **Solution**: Wrapped training loop in `if __name__ == '__main__':`
- **Location**: All `train_*.py` files

### Fix 2: OpenMP Library Conflict
- **Problem**: `OMP Error #15` - libomp.dll vs libiomp5md.dll conflict
- **Solution**: Added `os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"` at top
- **Location**: All `train_*.py` files

### Fix 3: Camera Matrix Tensor Batching
- **Problem**: DataLoader didn't batch 3Ã—3 camera matrices correctly
- **Solution**: Added dimension check in `pose_net_hybrid.py` to expand [3,3] â†’ [B,3,3]
- **Location**: `models/pose_net_hybrid.py` line ~132

---

## ðŸ“ Project Structure

```
Pose6D/
â”œâ”€â”€ PROJECT_STATUS.md           â† YOU ARE HERE
â”œâ”€â”€ README.md                   â† User-facing documentation
â”œâ”€â”€ colab_setup.ipynb           â† Google Colab deployment (27 cells)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_rgb.py          â† RGB-only (5 outputs)
â”‚   â”œâ”€â”€ dataset_rgbd.py         â† RGB-D (5 outputs)
â”‚   â””â”€â”€ dataset_hybrid.py       â† RGB-D + camera info (7 outputs)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pose_net_rgb.py         â† ResNet50
â”‚   â”œâ”€â”€ pose_net_rgbd.py        â† ResNet50 + ResNet50
â”‚   â”œâ”€â”€ pose_net_hybrid.py      â† ResNet50 + Custom CNN + Pinhole
â”‚   â””â”€â”€ loss.py                 â† ADD loss (with optional rot/trans weights)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ setup_data.py       â† Download LineMOD dataset
â”‚   â”‚   â”œâ”€â”€ setup_weights.py    â† Download pretrained weights
â”‚   â”‚   â””â”€â”€ prepare_yolo.py     â† Convert to YOLO format
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_yolo.py
â”‚   â”‚   â”œâ”€â”€ train_rgb.py
â”‚   â”‚   â”œâ”€â”€ train_rgbd.py
â”‚   â”‚   â””â”€â”€ train_hybrid.py     â† CURRENTLY RUNNING
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ inference_rgb.py
â”‚   â”‚   â””â”€â”€ inference_rgbd.py
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ visualize_yolo.py
â”‚       â”œâ”€â”€ visualize_rgb.py
â”‚       â”œâ”€â”€ visualize_rgbd.py
â”‚       â”œâ”€â”€ compare_rgb_vs_rgbd.py
â”‚       â””â”€â”€ compare_rgb_vs_hybrid.py
â”‚
â”œâ”€â”€ weights_rgb/                â† RGB model checkpoints
â”œâ”€â”€ weights_rgbd/               â† RGB-D model checkpoints
â”œâ”€â”€ weights_hybrid/             â† Hybrid model checkpoints (TRAINING)
â”‚   â”œâ”€â”€ best_pose_model.pth     â† Best validation loss
â”‚   â””â”€â”€ last_pose_model.pth     â† Latest epoch (for resuming)
â”‚
â””â”€â”€ datasets/
    â””â”€â”€ Linemod_preprocessed/
        â”œâ”€â”€ data/               â† 13 object folders (01-15, no 03,07)
        â”‚   â”œâ”€â”€ 01/
        â”‚   â”‚   â”œâ”€â”€ gt.yml      â† Ground truth poses
        â”‚   â”‚   â”œâ”€â”€ info.yml    â† Camera intrinsics (fx, fy, cx, cy)
        â”‚   â”‚   â”œâ”€â”€ train.txt
        â”‚   â”‚   â”œâ”€â”€ test.txt
        â”‚   â”‚   â”œâ”€â”€ rgb/
        â”‚   â”‚   â”œâ”€â”€ depth/
        â”‚   â”‚   â””â”€â”€ mask/
        â”‚   â””â”€â”€ ... (02, 04-06, 08-15)
        â””â”€â”€ models/
            â”œâ”€â”€ models_info.yml â† 3D model info (diameter, etc.)
            â””â”€â”€ obj_*.ply       â† 3D point clouds for ADD metric
```

---

## ðŸ”„ Data Pipeline

1. **YOLO Detection** â†’ Bounding boxes around objects
2. **Crop RGB + Depth** â†’ Extract 224Ã—224 patches
3. **Load Ground Truth** â†’ Quaternion (4D) + Translation (3D) from `gt.yml`
4. **Load Camera Info** â†’ fx, fy, cx, cy from `info.yml` (hybrid model only)
5. **Augmentation** â†’ ColorJitter + Bbox jitter (NO rotation/translation noise)
6. **Depth Processing**:
   - Bilateral filter: `cv2.bilateralFilter(depth_mm, 5, 75, 75)`
   - Convert to meters: `/ 1000.0`
   - Clip and normalize: `/ 1.5` (LineMOD max depth ~1.5m)

---

## ðŸ“‹ TODO List (Priority Order)

### ðŸ”¥ Immediate (Training Campaign)
1. **Option A - Resume Hybrid**: `python scripts/training/train_hybrid.py` (will auto-resume from epoch 2)
2. **Option B - Fresh Start**: Systematic retraining RGB â†’ RGB-D â†’ Hybrid
3. After any training completes: Run comparison scripts to evaluate improvements

### ðŸŽ¯ High Priority (Complete Retraining)
4. Train RGB model with fixed augmentation
   - Command: `python scripts/training/train_rgb.py`
   - Expected: ~5-5.5cm (down from 6.5cm buggy version)
5. Train RGB-D model with fixed augmentation
   - Command: `python scripts/training/train_rgbd.py`
   - Expected: ~3-3.5cm (down from 4.2cm buggy version)
6. Complete Hybrid training (100 epochs)
   - Command: `python scripts/training/train_hybrid.py`
   - Expected: ~3cm (best of all approaches)
7. Run full comparison: `python scripts/visualization/compare_rgb_vs_rgbd.py` and `compare_rgb_vs_hybrid.py`

### ðŸ“¦ Medium Priority (Deployment)
8. Package new pre-trained weights
9. Upload to Google Drive
10. Update Colab notebook links
11. Update README.md with final results

### ðŸš€ Low Priority (Optimization)
12. Model optimization experiments:
    - Mixed precision training (2x faster)
    - Differential learning rates (faster/slower branches)
    - Switch ResNet50 â†’ ResNet34 (30% smaller)
13. Model quantization (INT8) for deployment
14. ONNX export for cross-platform inference

---

## ðŸ§ª Experiments Tried & Abandoned

### Geometric Pose Estimation (ABANDONED)
- **Idea**: Use depth + 2D-3D correspondence for pose via PnP
- **Result**: 15cm translation error (worse than 6.5cm learned)
- **Conclusion**: End-to-end learning superior for this task
- **Files deleted**: `geometric_pose.py`, etc.

### Depth Processing Fixes (APPLIED)
- **Issue 1**: Bilateral filter sigma too low (5) â†’ changed to 75 (mm-scale values)
- **Issue 2**: Depth normalization range wrong (3m) â†’ changed to 1.5m (LineMOD-specific)
- **Result**: Improved depth features

---

## ðŸ’¡ Key Insights

1. **Hybrid Model Fast Convergence**: 4.3cm by epoch 2 because X,Y are geometrically computed from Z
2. **Domain Knowledge Helps**: Incorporating pinhole camera model reduces learning complexity
3. **Augmentation is Critical**: Wrong augmentation destroyed performance - fixed now
4. **Depth Matters**: RGB-D (4.2cm) beats RGB (6.5cm) by 35%
5. **Pure ADD Loss Works Best**: Separate rotation/translation weights (rot_weight=0.0, trans_weight=0.0) performed worse
6. **AI Agent Documentation**: Created `.github/copilot-instructions.md` for systematic knowledge transfer - covers architecture, conventions, Windows fixes, and development patterns

---

## ðŸ› ï¸ How to Resume/Continue

### If Training Interrupted
```bash
cd "D:\MSc\Year2Semester1\Data Analysis and Artificial Intellegence\Projects\Pose6D"
python scripts/training/train_hybrid.py
```
- Will automatically resume from `weights_hybrid/last_pose_model.pth`

### If Starting Fresh Agent
1. Read this file (PROJECT_STATUS.md)
2. Check `weights_hybrid/` for checkpoints
3. Look at terminal output for last epoch number
4. Review `colab_setup.ipynb` for deployment details
5. Check `scripts/training/train_hybrid.py` for current hyperparameters

### To Visualize Results
```bash
# After training completes
python scripts/visualization/compare_rgb_vs_hybrid.py
```

---

## ðŸ“Š Dataset Info

- **Name**: LineMOD (Hinterstoisser et al.)
- **Objects**: 13 household objects (ape, benchvise, cam, can, cat, driller, duck, eggbox, glue, holepuncher, iron, lamp, phone)
- **Images**: ~1200 per object
- **Resolution**: RGB 640Ã—480, Depth 640Ã—480
- **Split**: ~80% train, ~20% val
- **Camera**: Fixed intrinsics (fx, fy, cx, cy in info.yml)
- **Pose Format**: Quaternion (wxyz) + Translation (xyz in meters)

---

## ðŸ”— Important References

- **Google Drive**: Pre-trained weights hosted (see colab_setup.ipynb)
- **Colab Badge**: "Open in Colab" button in README.md
- **ADD Metric**: Average Distance of 3D model points after pose transformation
- **LineMOD Paper**: Hinterstoisser et al., "Model Based Training, Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes"

---

## ðŸš¨ Known Issues & Solutions

| Issue | Solution | Status |
|-------|----------|--------|
| Windows multiprocessing error | Add `if __name__ == '__main__':` wrapper | âœ… Fixed |
| OpenMP library conflict | Set `os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"` | âœ… Fixed |
| Camera matrix batching | Add dimension check in forward() | âœ… Fixed |
| Buggy augmentation | Remove rotation/translation noise | âœ… Fixed (need retrain) |
| Depth processing wrong | Bilateral sigma=75, max_depth=1.5m | âœ… Fixed |

---

## ðŸ“ Notes for Agent

- **Current Priority**: Wait for hybrid training to complete, then compare approaches
- **Don't Retrain Yet**: RGB/RGB-D models need retraining with fixed augmentation, but wait until hybrid results are analyzed
- **Checkpoint Path**: `weights_hybrid/last_pose_model.pth` has epoch, model, optimizer, best_val_loss
- **Training Time**: ~100 epochs Ã— 2 min/epoch = ~3-4 hours remaining
- **Next Script**: `compare_rgb_vs_hybrid.py` after training completes
- **User's Main Question**: Does hybrid (learned + geometric) beat pure learning (RGB-D)?

---

**END OF STATUS DOCUMENT**
